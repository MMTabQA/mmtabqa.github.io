<!DOCTYPE html>
<html lang="en-us">
	<head>
		<meta charset="UTF-8">
		<title>MMTabQA</title>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="theme-color" content="#157878">
		<link rel="stylesheet" href="css/normalize.css">
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
		<link rel="stylesheet" href="css/cayman.css">
	</head>
	<body>
		<section class="page-header">
			<h1><img src="figures/logo.jpg" style="max-width:40%;"></h1> 
			<a href="https://arxiv.org/pdf/2408.13860" class="btn">Paper</a>
			<a href="https://github.com/MMTabQA/mmtabqa" class="btn">Dataset</a>
			<a href="explore.html" class="btn">Explore</a>
			<a href="https://github.com/MMTabQA/mmtabqa" class="btn">Code</a> 
			<a href="https://mmtabqa.github.io" class="btn">Video</a>
			<a href="https://docs.google.com/presentation/d/1DQpaDDDMZBwy75SoBp_GwD_5vlVqkTbgIOyEWEep2qY/edit?usp=sharing" class="btn">PPT</a><br>
			<a href="https://mmtabqa.github.io" class="btn">Poster</a><br>
		</section>
		<section class="main-content">
			<h1>Knowledge-Aware Reasoning over Multimodal Semi-structured Tables</h1>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2><p style="text-align: justify;"> Existing datasets for tabular question answering typically focus exclusively on text within cells. However, real-world data is inherently multimodal, often blending images such as symbols, faces, icons, patterns, and charts with textual content in tables. With the evolution of AI models capable of multimodal reasoning, it is pertinent to assess their efficacy in handling such structured data.<p>
			<p style="margin-left:10%; margin-right:10%;"><img src="figures/iphone.png" style="max-width:95%;"></p>
			<p style="text-align: justify;">  This study investigates whether current AI models can perform knowledge-aware reasoning on multimodal structured data. We explore their ability to reason on tables that integrate both images and text, introducing {\sc {\sc MMTabQA}}, a new dataset designed for this purpose. Our experiments highlight substantial challenges for current AI models in effectively integrating and interpreting multiple text and image inputs, understanding visual context, and comparing visual content across images. These findings establish our dataset as a robust benchmark for advancing AI's comprehension and capabilities in analyzing multimodal structured data.<p>

			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Details</h2>
			<p style="text-align: justify;">we create the MMTabQA dataset with 69,740 questions over 25,026 tables by augmenting existing tables from four data sources:
				<ul>
					<li><strong>WikiSQL Dataset</strong> to benchmark model capabilities in parsing entities accurately and answering basic SQL-based questions.</li>
					<li><strong>WikiTableQuestions dataset</strong> to include questions which require more complex reasoning.</li>
					<li><strong>FeTaQA dataset</strong> to include long-form answer-based questions which involve multiple row/column reasoning.</li>
					<li><strong>HybridQA dataset</strong> which includes extra contextual passages beyond the tables, requiring hybrid complex reasoning.</li>
				</ul>
			</p>
			<p style="text-align: justify;">The dataset contains questions of the following types:
				<ul>
					<li><strong>Explicit Questions</strong>: which mention an entity that is replaced by an image in the table.</li>
					<li><strong>Answer-Mention Questions</strong>: whose answer contains an entity that is replaced by an image in the table, but the question does not.</li>
					<li><strong>Implicit Questions</strong>: where an image-replaced entity is involved in intermediate reasoning but not mentioned in the answer or the question.</li>
					<li><strong>Visual Questions</strong>: Questions that involve visual aspects of entities. These are created by synthetically augmenting the explicit questions in the dataset with visual attributes of the entity they refer to.</li>
				</ul>
				
			<div>
				<table style="margin-left:10%;margin-right:10%;text-align: center">
					<col>
					<colgroup span="2"></colgroup>
					<colgroup span="2"></colgroup>
					<tr>
					  <th scope="col">No. of Questions</th>
					  <th scope="col">Avg. Img per Table</th>
					  <th scope="col">No. of Tables</th>
					  <th scope="col">Overall</th>
					</tr>
					<tr>
					  <th scope="row"><strong>WikiSQL</strong></th>
					  <td>21,472</td>
					  <td>13.68</td>
					  <td>9,784</td>
					</tr>
					<tr>
					  <th scope="row"><strong>WikiTable-Questions</strong></th>
					  <td>10,052</td>
					  <td>17.67</td>
					  <td>1,259</td>
					</tr>
					<tr>
					  <th scope="row"><strong>FeTaQA</strong></th>
					  <td>7,476</td>
					  <td>10.43</td>
					  <td>5,898</td>
					</tr>
					<tr>
					  <th scope="row"><strong>HybridQA</strong></th>
					  <td>30,470</td>
					  <td>14.64</td>
					  <td>8,085</td>
					</tr>
					<tr>
					  <th scope="row"><strong>Overall</strong></th>
					  <td>69,740</td>
					  <td>14.10</td>
					  <td>25,026</td>
					</tr>
				  </table>
			     <caption>Table: <strong>Primary Dataset Statistics</strong>
			</div>
			<div>
				<table style="margin-left:0%;margin-right:0%;text-align: center">
					<col>
					<colgroup span="4"></colgroup>
					  <td rowspan="2"><strong>Data Source</strong></td>
					  <th colspan="4" scope="colgroup">Question Type Statistics</th>
					</tr>
					<tr>
					  <th scope="col"><strong>Explicit Ques</strong></th>
					  <th scope="col"><strong>Implicit Ques</strong></th>
					  <th scope="col"><strong>Visual Ques</strong></th>
					  <th scope="col"><strong>Answer-Mention Ques</strong></th>
					</tr>
					<tr>
					  <th scope="row"><strong>FeTaQA</strong></th>
					  <td>2,499</td>
					  <td>612</td>
					  <td>1,185</td>
					  <td>3,180</td>
					</tr>
					<tr>
					  <th scope="row"><strong>WikiTable-Questions</strong></th>
					  <td>3,523</td>
					  <td>2,879</td>
					  <td>877</td>
					  <td>2,773</td>
					</tr>
					<tr>
					  <th scope="row"><strong>WikiSQL</strong></th>
					  <td>12,956</td>
					  <td>315</td>
					  <td>1,827</td>
					  <td>6,374</td>
					</tr>
					<tr>
					  <th scope="row"><strong>HybridQA</strong></th>
					  <td>5,819</td>
					  <td>17,647</td>
					  <td>1,874</td>
					  <td>5,130</td>
					</tr>
				  </table>
				  
                                <caption><strong>Question Type Statistics: </strong>Distribution of the different types of questions in the dataset.</caption>
			</div>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Creation</h2>
			<p>The figure below shows the steps followed for creating the dataset:</p>
			<p style="margin-left:20%; margin-right:20%;"><img src="figures/dataset_diagram.png" style="max-width:95%;"></p>
			
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Examples</h2>
			<figure>
				<img src="figures/ex_1.jpg">
				<img src="figures/ex_2.jpg">
				<img src="figures/ex_3.png">
				<img src="figures/ex_4.png">

			</figure>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Results</h2>
			<div class="small-font">
			<table>
				
				<thead>
					<tr>
						<th>Dataset</th>
						<th colspan="4">WikiTableQuestions</th>
						<th colspan="4">WikiSQL</th>
						<th colspan="4">FetaQA</th>
					</tr>
					<tr>
						<th>Model</th>
						<th>EQ</th>
						<th>AQ</th>
						<th>IQ</th>
						<th>VQ</th>
						<th>EQ</th>
						<th>AQ</th>
						<th>IQ</th>
						<th>VQ</th>
						<th>EQ</th>
						<th>AQ</th>
						<th>IQ</th>
						<th>VQ</th>
					</tr>
				</thead>
				<tbody>
					<tr class="midrule">
						<td colspan="13">Partial Input Baseline</td>
					</tr>
					<tr>
						<td>Gemini-1.5 Flash</td>
						<td>40.99</td>
						<td>27.38</td>
						<td>48.95</td>
						<td>31.4</td>
						<td>39.14</td>
						<td>28.71</td>
						<td>62.22</td>
						<td>28</td>
						<td class="highlight">0.51</td>
						<td>0.44</td>
						<td class="highlight">0.44</td>
						<td class="highlight">0.47</td>
					</tr>
					<tr>
						<td>GPT-4o</td>
						<td class="highlight">57.45</td>
						<td class="highlight">38.02</td>
						<td class="highlight">70.83</td>
						<td class="highlight">42.40</td>
						<td class="highlight">52.57</td>
						<td class="highlight">43.86</td>
						<td class="highlight">72.38</td>
						<td class="highlight">39.00</td>
						<td>0.51</td>
						<td class="highlight">0.46</td>
						<td>0.42</td>
						<td>0.44</td>
					</tr>
					<tr>
						<td>Llama-3 70B</td>
						<td>41.13</td>
						<td>26.48</td>
						<td>43.75</td>
						<td>31.8</td>
						<td>41.117</td>
						<td>30.75</td>
						<td>61.27</td>
						<td>30.6</td>
						<td>0.52</td>
						<td>0.46</td>
						<td>0.45</td>
						<td>0.48</td>
					</tr>
					<tr>
						<td>Mixtral 8x7B</td>
						<td>26.56</td>
						<td>9.90</td>
						<td>30.26</td>
						<td>20.2</td>
						<td>23.42</td>
						<td>17.71</td>
						<td>28.88</td>
						<td>19.2</td>
						<td>0.44</td>
						<td>0.39</td>
						<td>0.38</td>
						<td>0.39</td>
					</tr>
					<tr class="midrule">
						<td colspan="13">Oracle-Entity Replaced Baseline</td>
					</tr>
					<tr>
						<td>Gemini-1.5 Flash</td>
						<td>74.89</td>
						<td>78.19</td>
						<td>54.86</td>
						<td>-</td>
						<td>82.28</td>
						<td>81.86</td>
						<td>77.46</td>
						<td>-</td>
						<td class="highlight">0.56</td>
						<td class="highlight">0.50</td>
						<td>0.41</td>
						<td>-</td>
					</tr>
					<tr>
						<td>GPT-4o</td>
						<td class="highlight">87.80</td>
						<td class="highlight">84.86</td>
						<td class="highlight">84.55</td>
						<td>-</td>
						<td class="highlight">85.57</td>
						<td class="highlight">82.71</td>
						<td class="highlight">79.05</td>
						<td>39.00</td>
						<td>0.53</td>
						<td>0.48</td>
						<td class="highlight">0.43</td>
						<td>-</td>
					</tr>
					<tr>
						<td>Llama-3 70B</td>
						<td>75.74</td>
						<td>75.31</td>
						<td>58.85</td>
						<td>-</td>
						<td>78.28</td>
						<td>78.57</td>
						<td>68.25</td>
						<td>-</td>
						<td>0.49</td>
						<td>0.46</td>
						<td>0.41</td>
						<td>-</td>
					</tr>
					<tr>
						<td>Mixtral 8x7B</td>
						<td>54.89</td>
						<td>53.87</td>
						<td>40.69</td>
						<td>-</td>
						<td>59.28</td>
						<td>69.28</td>
						<td>33.96</td>
						<td>-</td>
						<td>0.44</td>
						<td>0.41</td>
						<td>0.33</td>
						<td>-</td>
					</tr>
					<tr class="midrule">
						<td colspan="13">Image-Captioning Baseline</td>
					</tr>
					<tr>
						<td>Gemini-1.5 Flash</td>
						<td>52.34</td>
						<td>42.16</td>
						<td>51.39</td>
						<td>42.2</td>
						<td>50.42</td>
						<td>40.85</td>
						<td>67.30</td>
						<td>46.6</td>
						<td>0.57</td>
						<td>0.46</td>
						<td>0.42</td>
						<td>0.43</td>
					</tr>
					<tr class="midrule">
						<td colspan="13">Table-as-an-Image Baseline</td>
					</tr>
					<tr>
						<td>Gemini-1.5 Flash</td>
						<td>44.22</td>
						<td>25.65</td>
						<td>41.01</td>
						<td>37.8</td>
						<td>47.08</td>
						<td>35.75</td>
						<td>52.38</td>
						<td>35.25</td>
						<td>0.62</td>
						<td>0.43</td>
						<td>0.42</td>
						<td>0.47</td>
					</tr>
					<tr>
						<td>GPT-4o</td>
						<td>66.12</td>
						<td>50.60</td>
						<td>62.12</td>
						<td>46.0</td>
						<td>72.28</td>
						<td>64.82</td>
						<td>58.49</td>
						<td>32.6</td>
						<td>0.62</td>
						<td>0.46</td>
						<td>0.43</td>
						<td>0.50</td>
					</tr>
					<tr>
						<td>Llama-3 70B</td>
						<td>57.76</td>
						<td>41.90</td>
						<td>45.54</td>
						<td>42.1</td>
						<td>55.24</td>
						<td>48.54</td>
						<td>59.84</td>
						<td>45.8</td>
						<td>0.56</td>
						<td>0.45</td>
						<td>0.41</td>
						<td>0.44</td>
					</tr>
					<tr>
						<td>Mixtral 8x7B</td>
						<td>40.34</td>
						<td>20.38</td>
						<td>41.16</td>
						<td>30.9</td>
						<td>38.74</td>
						<td>29.29</td>
						<td>39.12</td>
						<td>30.9</td>
						<td>0.55</td>
						<td>0.43</td>
						<td>0.41</td>
						<td>0.42</td>
					</tr>
				</tbody>
			</table>
			</div>
			<caption>Results on sampled subset of MMTabQA. Substring match is reported for Wiki-related data sources and ROUGE-L is reported for FetaQA data source. EQ - Explicit Questions, AQ - Answer-Mention Questions, IQ - Implicit Questions, VQ - Visual Questions. Best performing models are highlighted in red.</caption>
						<!-- <br><br>
			<div style="text-align:center;">
				<table>
					<thead>
						<tr>
							<th>Data Split</th>
							<th>Cohen's Kappa</th>
							<th>Human Performance</th>
							<th>Majority Agreeement</th>
						</tr>
					</thead>
					<tbody align="center">
						<tr>
							<td>Dev</td>
							<td>0.78</td>
							<td>79.78</td>
							<td>93.53</td>
						</tr>
						<tr>
							<td>alpha 1</td>
							<td>0.80</td>
							<td>84.04</td>
							<td>97.48</td>
						</tr>
						<tr>
							<td>alpha 2</td>
							<td>0.80</td>
							<td>83.88</td>
							<td>96.77</td>
						</tr>
						<tr>
							<td>alpha 3</td>
							<td>0.74</td>
							<td>79.33</td>
							<td>95.58</td>
						</tr>
					</tbody>
					<caption>Cohen's Kappa, human baseline and inter-annotator agreement scores</caption>
				</table>
			</div> -->
			<!-- <h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Knowledge + InfoTabS</h2>
			<p style="text-align: justify;"> You should check our <a href="https://2021.naacl.org/">NAACL 2021</a> paper which <a href="https://knowledge-infotabs.github.io">enhance InfoTabS</a> with extra Knowledge.</p>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>TabPert</h2>
			<p style="text-align: justify;"> You should check our <a href="https://2021.emnlp.org">EMNLP 2021</a> paper which is a <a href="https://tabpert.github.io">tabular perturbation platform</a> to generate counterfactual examples.</p> -->

			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>People</h2>
			<p style="text-align: justify;"> The InfoSync dataset is prepared by collaboration of across multiple institutions <a href="https://www.cs.utah.edu/">University of Utah</a>, <a href="https://www.iitg.ac.in/cse/"> IIT Guwahati</a>,<a href="https://www.ctae.ac.in/"> CTAE</a>   and <a href="https://www.bloomberg.com/company/"> Bloomberg LP</a> by the following people: </p>

			<figure style="display: flex; flex-wrap: wrap; justify-content: center;">
          <a href="https://www.linkedin.com/in/suyashmathur/">
            <img src="figures/suyash.jpg" style="width:150px; height: 150px; margin: 10px;">
            <figcaption>Suyash Mathur</figcaption>
        </a>
        <a href="https://www.linkedin.com/in/bjainit/">
          <img src="figures/jainit.jpeg" style="width:150px; height: 150px; margin: 10px;">
          <figcaption>Jainit Bafna</figcaption>
      </a>
		<a href="https://www.linkedin.com/in/kunalkartik/">
			<img src="figures/kunal.jpeg" style="width:150px; height: 150px; margin: 10px;">
			<figcaption>Kunal Kartik</figcaption>
		</a>
		<a href="https://www.linkedin.com/in/harshita-31011999/">
		<img src="figures/harshita.jpeg" style="width:150px; height: 150px; margin: 10px;">
		<figcaption>Harshita Khandelwal</figcaption>
	</a>
	<a href="https://www.iiit.ac.in/faculty/manish-shrivastava/">
		<img src="figures/manish.jpeg" style="width:150px; height: 150px; margin: 10px;">
		<figcaption>Manish Shrivastava</figcaption>
	</a>
	<a href="https://vgupta123.github.io">
	<img src="figures/vivekg.jpg" style="width:150px; height: 150px; margin: 10px;">
	<figcaption>Vivek Gupta</figcaption>
	</a>
	<a href="https://www.cs.unc.edu/~mbansal/">
		<img src="figures/mohit.jpeg" style="width:150px; height: 150px; margin: 10px;">
		<figcaption>Mohit Bansal</figcaption>
		</a>
	<a href="https://www.cis.upenn.edu/~danroth/">
	<img src="figures/dan.jpeg" style="width:150px; height: 150px; margin: 10px;">
	<figcaption>Dan Roth</figcaption>
	</a>
      </figure> 
			<!-- <figure>
				<img src="figures/suyash.jpg" width="140" height="120">
				<img src="figures/jainit.jpeg" width="140" height="120">
				<img src="figures/kunal.jpg" width="140" height="120">
				<img src="figures/harshita.jpeg" width="140" height="120">
				<img src="figures/vivekg.jpg" width="140" height="120">
				<img src="figures/manish.jpeg" width="140" height="120">
				<img src="figures/mohit.jpeg" width="140" height="120">
				<img src="figures/dan.jpeg" width="140" height="120">

				<figcaption>From left to right <a href="https://www.linkedin.com/in/siddharth-khincha-644a70203">Siddharth Khincha</a>,<a href="https://www.linkedin.com/in/chelsi-jain-7b0734192">Chelsi Jain</a>, <a href="https://vgupta123.github.io">Vivek Gupta*</a>,<a href="https://tushaarkataria.github.io/">Tushar Kataria*</a> and <a href="https://imsure318.github.io/">Shuo Zhang</a>. </figcaption>
			</figure> -->
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h2>
			<p style="text-align: justify;"> Please cite our paper as below if you use the InfoSync dataset.</p>
			<pre><code>@misc{mathur2024knowledgeawarereasoningmultimodalsemistructured,
	title={Knowledge-Aware Reasoning over Multimodal Semi-structured Tables}, 
	author={Suyash Vardhan Mathur and Jainit Sushil Bafna and Kunal Kartik and Harshita Khandelwal and Manish Shrivastava and Vivek Gupta and Mohit Bansal and Dan Roth},
	year={2024},
	eprint={2408.13860},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2408.13860}, 
}
</code></pre>
			<h2><a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Acknowledgement</h2>
			<p style="text-align: justify;">Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied,
				of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This work was partially funded by ONR Contract N00014-19-1-2620. Lastly, we extend our appreciation to the reviewing team
				for their insightful comments.
			<footer class="site-footer">
				<span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman</a> theme by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
			</footer>
		</section>
	</body>
</html>
